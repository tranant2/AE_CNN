#!/bin/bash --login
########## SBATCH Lines for Resource Request ##########
 
#SBATCH --time=24:00:00             # limit of wall clock time - how long the job will run (same as -t)
#SBATCH --nodes=1                 # number of different nodes - could be an exact number or a range of nodes (same as -N)
#SBATCH --ntasks=1                  # number of tasks - how many tasks (nodes) that you require (same as -n)
#SBATCH -c 16                      # number of cpu per task
#SBATCH --job-name GPU_9      # you can give your job a name for easier identification (same as -J)
#SBATCH --mem=0
#SBATCH --gres=gpu:k80:1  
 
########## Command Lines for Job Running ##########
echo "JobID: $SLURM_JOB_ID"
echo "Time: `date`"
echo "Running on node: `hostname`"
echo "Current directory: `pwd`"

#module load GCC/10.2.0 
#module load CUDA/11.1.1
#module load OpenMPI/4.0.5
#module load PyTorch/1.8.1
#module load HDF5/1.10.7
#ml -* GCCcore/8.3.0 Python/3.8.3

echo "Running Code"
echo $CUDA_VISIBLE_DEVICES

cd /mnt/home/tranant2/Desktop/MachineLearning/Environments
source cnn8/bin/activate

cd /mnt/home/tranant2/Desktop/MachineLearning/TRACK/CNN9
srun python CNN9__.py

scontrol show job $SLURM_JOB_ID     ### write job information to SLURM output file.
js -j $SLURM_JOB_ID                 ### write resource usage to SLURM output file (powertools command).
echo "Run Successful"
echo "Time: `date`"